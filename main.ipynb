{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen2-VL-7B-Instruct\n",
    "Qwen/Qwen2-VL-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install bitsandbytes peft trl\n",
    "# ! pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # weights and boases comes enabled by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    Qwen2VLForConditionalGeneration,\n",
    "    Qwen2VLProcessor,\n",
    "    BitsAndBytesConfig,  # quatinzation\n",
    ")\n",
    "\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")  # as we dont have enough gpu to work with full vision model so we are gpoing with adapaters such as LoRa\n",
    "\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device='cuda:0'\n",
      "NUM_STEPS=283\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"{device=}\")\n",
    "\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "GRADIENT_CHECKPOINTING = True\n",
    "USE_REENTRANT = False\n",
    "OPTIM = \"paged_adamw_32bit\"\n",
    "LEARNING_RATE = 2e-5\n",
    "LOGGING_STEPS = 50\n",
    "EVAL_STEPS = 50\n",
    "SAVE_STEPS = 50\n",
    "SAVE_STRATEGY = \"steps\"\n",
    "EVAL_STRATEGY = \"steps\"\n",
    "METRIC_FOR_BEST_MODEL = (\n",
    "    \"eval_loss\"  # to get better model which might not be the last saved model\n",
    ")\n",
    "LOAD_BEST_MODEL_AT_END = True\n",
    "MAX_GRAD_NORM = 1\n",
    "WARMUP_STEPS = 0\n",
    "DATASET_KWARGS = {\"skip_prepare_dataset\": True}  # we have to put for VLMs\n",
    "\n",
    "REMOVE_UNUSED_COLUMNS = False  # VLM thing\n",
    "MAX_SEQ_LEN = 128  # max seq len of the generated text\n",
    "DATA_POINTS_IN_DATASET = 283\n",
    "NUM_STEPS = (DATA_POINTS_IN_DATASET // BATCH_SIZE) * EPOCHS\n",
    "print(f\"{NUM_STEPS=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_message=' You are a highly advanced Vision Language Model (VLM), specializing in analyzing, describing and interpreting visual data.\\nYour task is to process and extract meaningful insights from images, videos and visual patterns,\\nleveraging multimodal understanding to provide accurate and contextually relevant infomration.'\n"
     ]
    }
   ],
   "source": [
    "# our data will need to have format that is compatible with VLM and SIFT trainer\n",
    "# so for every data point in dataset we will the points in a fomrat that we want\n",
    "# for that first we'll display a system_message\n",
    "\n",
    "system_message = \"\"\" You are a highly advanced Vision Language Model (VLM), specializing in analyzing, describing and interpreting visual data.\n",
    "Your task is to process and extract meaningful insights from images, videos and visual patterns,\n",
    "leveraging multimodal understanding to provide accurate and contextually relevant infomration.\"\"\"\n",
    "print(f\"{system_message=}\")\n",
    "\n",
    "\n",
    "def format_data(sample):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
    "                {\"type\": \"text\", \"text\": sample[\"query\"]},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"][0]}],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7825e8b3ba6474e8c9bfae311c68717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/852 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7b9722f268446ead9a5ce6146aa257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00003-49492f364babfa44.parquet:   0%|          | 0.00/219M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fe95d510ff419cb497154adbffaf79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00001-of-00003-7302bae5e425bbc7.parquet:   0%|          | 0.00/311M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f808a2b88e34d91ad2890052847bec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00002-of-00003-194c9400785577a2.parquet:   0%|          | 0.00/315M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222101d0be9542e08ee10f64c0ac070d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-0f11003c77497969.parquet:   0%|          | 0.00/50.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612c31b534394231a6b57c2ab509f9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-e2cd0b7a0f9eb20d.parquet:   0%|          | 0.00/68.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3591994f0efc4d86baacf195a74f1030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/28299 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2748a2ab2df44768b70dbba62df6d25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split:   0%|          | 0/1920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40403acf25fc43778800178a47d99664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset, eval_dataset, test_dataset = load_dataset(\n",
    "    \"HuggingFaceM4/ChartQA\",\n",
    "    split=[\"train[:1%]\", \"val[:1%]\", \"test[:1%]\"],  # only using 1% of data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

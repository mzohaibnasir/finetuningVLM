{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen2-VL-7B-Instruct\n",
    "Qwen/Qwen2-VL-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install bitsandbytes peft trl\n",
    "# ! pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA: low rank adaption of LLMs\n",
    "\n",
    "## why we need LoRA?\n",
    "In nn, we have \n",
    "        \n",
    "        input -> hidden layer1 -> hidden layer2 -> output\n",
    "        then we campare output and target for loss and then backpropagation to each of the weights of all the layers and each of these weights will modified by the loss function\n",
    "\n",
    "![alt-txt](finetuning_01.png)\n",
    "In finetuning we have \n",
    "        \n",
    "        a pretrained model and we want to finetune it on some other data that original model might not have seen. \n",
    "\n",
    "problem with fine tuning,\n",
    "\n",
    "        **Computational Cost**\n",
    "\n",
    "            Requires updating all model parameters\n",
    "            High GPU/TPU requirements\n",
    "            Expensive for large models\n",
    "\n",
    "\n",
    "        **Memory Issues**\n",
    "\n",
    "            Full model weights must fit in GPU memory\n",
    "            Large batch sizes often impossible\n",
    "\n",
    "\n",
    "        **Catastrophic Forgetting**\n",
    "\n",
    "            Model may lose previously learned knowledge\n",
    "            Can degrade performance on original tasks\n",
    "\n",
    "\n",
    "## LoRA\n",
    "In LoRA, we start wtht input than we have our pretrained model and we freeze it weight, basically we tell pytorch to never touch these weights; just use them as read-only, never run back propagation on them.\n",
    "Then we create two other matrices A,B(basically in LoRA we dont have to create matrices A and B for each layer, wecan just do it for some layers).\n",
    "Suppose we only have one layer, we introduced the matrices A and B\n",
    "\n",
    "so what's the differece betwenn A and B and original(pre trained) matrix W?\n",
    "\n",
    "    1. dimension-wise: W is dxk i.e. 1000, 5000; we want to create two new matrices A and B so that multiplied together they produce same dimension `dxk`\n",
    "        W: dxk => 1000*5000 -> 5 000000 parameters\n",
    "        \n",
    "        B: dxr\n",
    "        A: rxk\n",
    "        B.A: dxk   where r is much smaller than d and k so supose r =1, parameters = 1000+ 5000 =>  6000\n",
    "\n",
    "\n",
    "        this matrix may not capture the same infomration as original matrix W as its much smaller even though they produce same dimensions, this is smaller representation. so you loose some infomation.\n",
    "\n",
    "\n",
    "\n",
    "    Idea behnd LoRA is that : matrix W contains a lot of weights that are meaningful for our purposes, they are actually not adding any information to the model; they are just combination of the other weights so they are redundant. So, we dont need whole matrix W . we can create a lower representation of this W and finetune that one.\n",
    "\n",
    "![alt-txt](finetuning_02.png)(\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**we create two matrices B and A then we combine them with W, because we can sum them because both have same dimensions shape(BA) = shape(W), then we produce the output.**\n",
    "then we have our usual target(ground truth/label) to compute loss against output. then we only back propagate the loss along the matrix that we want to train B and A. so we never touched the W matrix(original model\n",
    "\n",
    "![alt-txt](finetuning_03.png))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### benfits\n",
    "    1. less parameter to train\n",
    "        if d=1000, k =5000 => d*k = 5000000 \n",
    "        using r=5:\n",
    "        we get (dxr) + (rxk) => 5000 + 25000 = 30000.\n",
    "    2. less parameters = less storage requirements\n",
    "    3. fast backpropagation, we do not need to evaluate the graident of the most oof parameters.\n",
    "    4. while woriking with two differnet fine tuned model(one for java , one for sql) just by chaingingthe parameters of A and B matrices instead of reloading W again becasue W was never touched.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA\n",
    "Even though LLMs are enormous, the changes you need to make to them to adapt them for a specific task are surprisingly structures.\n",
    "Rather than modifying all weights, LoRA inserts small trainsble parameters into each layer of the model. like Instead of rebuilding entire engne of race car, LoRA adds carefully designed tweeks.\n",
    "It not only reduces model checkpoint size from 350gb to 35 mb but also enhances perfommance.\n",
    "\n",
    "\n",
    "\n",
    "# the ideal tweak to the model, often has  a very low intrinsic rank. here rank is the measure of complexity of adjustment. meaning very low rank meaning very low adjustment can lead to significant improvemnt. Igt's not about making small tweaks but smart target tweaks.\n",
    "\n",
    "\n",
    "# if these simple adujustments can unock so much potential, it means their is adeeper structure to how these models learn and represent knowledge.\n",
    "\n",
    "# its like having really large library, but nnot knowing how to find the right books. LoRA givrs you perfect index guiding you to exact book.\n",
    "\n",
    "# researches found out that applying LORA to both query(questions) and value(answers) matrices in self attention yielded the best perofmmances.adapting just one or other wasn't jus t as effective.\n",
    "\n",
    "# rather than adding new info, LORA just amplifies alrady existig parameters i.e. boost in features. so LoRA is just liken a splotlight that highlights the specific knowledge needed for that task at hand.\n",
    "\n",
    "\n",
    "![alt-txt](finetuning_04.png)\n",
    "\n",
    "\n",
    "pretrained models have a low intrinsic dimesnion and can still learn efficiently despite a random projection to a smaller subspace. Inspired by this, hypotheseis is the updates of weights also have low intrinsic rank during adaption.\n",
    "\n",
    "### rank of maatrix is number of vectors that are linearly independent in that matrix i..e you can not combine any of them kinearly to produce an another one. It also indicates how many cikumns are redundant because all redunadnat columns can be obtained by linearly combining others.\n",
    "\n",
    "## So, W matrix is actually rank deficient whcih mean it doesn't have full rank. i.e. dimenions maybe 1000*1000 but actual rankg is 10, so we can use capture most of information by using 10x10 matrix.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This image provides insights into the concept of **low-rank parameterized update matrices** in fine-tuning pre-trained language models. Here's a breakdown of the highlighted ideas:\n",
    "\n",
    "1. **Intrinsic Dimension in Pre-trained Models**:  \n",
    "   The work by Aghajanyan et al. (2020) demonstrates that pre-trained language models possess a low \"intrinsic dimension.\" This means they can adapt to tasks efficiently even when projected into a smaller subspace, suggesting that their underlying structure doesn't require the full parameter space for effective learning.\n",
    "\n",
    "2. **Hypothesis on Intrinsic Rank in Adaptation**:  \n",
    "   Inspired by the above, the authors hypothesize that the **updates to the weight matrices** during task adaptation also have a **low \"intrinsic rank.\"** This aligns with the idea that a smaller rank suffices to capture the essential changes needed for adaptation.\n",
    "\n",
    "3. **Mathematical Framework**:  \n",
    "   - The weight matrix \\( W_0 \\in \\mathbb{R}^{d \\times k} \\) of a pre-trained model is constrained during adaptation by decomposing updates as \\( \\Delta W = B A \\), where \\( B \\in \\mathbb{R}^{d \\times r} \\) and \\( A \\in \\mathbb{R}^{r \\times k} \\).  \n",
    "   - The rank \\( r \\) is much smaller than \\( \\min(d, k) \\), meaning the updates are **rank-deficient**.  \n",
    "   - \\( W_0 \\) remains frozen during training, and only \\( A \\) and \\( B \\) are trainable.\n",
    "\n",
    "4. **Implication**:  \n",
    "   By adopting this low-rank update approach, models retain efficiency and performance while drastically reducing the number of parameters being fine-tuned. This aligns with efficient fine-tuning methods like **LoRA (Low-Rank Adaptation)**.\n",
    "\n",
    "Would you like a deeper explanation of any specific part, such as its applications or derivation?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **In-depth Explanation of LoRA (Low-Rank Adaptation) with Mathematical Foundations**\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is an efficient fine-tuning method for large language models (LLMs). Instead of updating all the parameters of the model, LoRA introduces a small set of additional parameters that are fine-tuned to adapt the model to a specific task. This reduces computational requirements and storage while maintaining high performance. Let’s explore it step by step with the necessary mathematical background.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Key Idea Behind LoRA**\n",
    "\n",
    "LLMs are typically parameterized by extremely large matrices \\(W\\) in each layer. These matrices define the weights that transform inputs to outputs across layers. Fine-tuning traditionally involves updating all these parameters for a specific task, which is computationally expensive and storage-heavy.\n",
    "\n",
    "LoRA’s key observation: **The task-specific updates to these large matrices have low intrinsic rank.** This means that instead of modifying the full \\(W\\), the changes can be expressed as a low-rank decomposition.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Mathematical Formulation of LoRA**\n",
    "\n",
    "Let \\(W\\) be a weight matrix in the model with dimensions \\((d_{\\text{out}}, d_{\\text{in}})\\). During fine-tuning, instead of directly updating \\(W\\), LoRA expresses the update as:\n",
    "\n",
    "\\[ W' = W + \\Delta W \\]\n",
    "\n",
    "Where:\n",
    "- \\(W\\) is the pre-trained weight matrix (frozen during LoRA fine-tuning).\n",
    "- \\(\\Delta W\\) is the task-specific update matrix, which LoRA approximates using a low-rank decomposition.\n",
    "\n",
    "#### **Low-Rank Decomposition**\n",
    "The update \\(\\Delta W\\) is parameterized as:\n",
    "\n",
    "\\[ \\Delta W = AB^\\top \\]\n",
    "\n",
    "Where:\n",
    "- \\(A\\) is a matrix of dimensions \\((d_{\\text{out}}, r)\\), where \\(r\\) is the rank of the decomposition.\n",
    "- \\(B\\) is a matrix of dimensions \\((d_{\\text{in}}, r)\\).\n",
    "- \\(r\\) is much smaller than the dimensions of \\(W\\), making \\(\\Delta W\\) a low-rank approximation.\n",
    "\n",
    "#### **Effective Parameter Reduction**\n",
    "The number of parameters introduced by LoRA is:\n",
    "\n",
    "\\[ \\text{Parameters in } \\Delta W = r \\cdot (d_{\\text{out}} + d_{\\text{in}}) \\]\n",
    "\n",
    "This is significantly smaller than the full \\(d_{\\text{out}} \\cdot d_{\\text{in}}\\) parameters of \\(W\\).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Training with LoRA**\n",
    "\n",
    "During fine-tuning:\n",
    "- \\(W\\) is frozen, meaning it is not updated.\n",
    "- Only the parameters \\(A\\) and \\(B\\) are trained, which significantly reduces computational costs.\n",
    "\n",
    "The forward pass with LoRA becomes:\n",
    "\n",
    "\\[ y = Wx + (AB^\\top)x \\]\n",
    "\n",
    "Where:\n",
    "- \\(x\\) is the input vector of size \\(d_{\\text{in}}\\).\n",
    "- \\(y\\) is the output vector of size \\(d_{\\text{out}}\\).\n",
    "\n",
    "#### **Backpropagation**\n",
    "Only the gradients for \\(A\\) and \\(B\\) are computed during backpropagation, while \\(W\\) remains unchanged. This further reduces the memory and computational overhead.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Intrinsic Rank and Task-Specific Adaptation**\n",
    "\n",
    "#### **Intrinsic Rank Hypothesis**\n",
    "LoRA is based on the hypothesis that task-specific updates to a pre-trained LLM are inherently low-rank. This is because:\n",
    "- Pre-trained models already encode a vast amount of general knowledge.\n",
    "- Fine-tuning for a specific task typically requires only minor adjustments to highlight the relevant features.\n",
    "\n",
    "The rank \\(r\\) determines the expressiveness of the update. Empirical studies have shown that even very small values of \\(r\\) (e.g., \\(r = 4\\) or \\(r = 8\\)) can lead to significant performance improvements.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Application in Self-Attention Mechanism**\n",
    "\n",
    "LoRA has been particularly effective when applied to the **query** and **value** matrices in the self-attention mechanism of transformers.\n",
    "\n",
    "#### **Attention Mechanism Overview**\n",
    "In transformers, the self-attention mechanism computes:\n",
    "\n",
    "\\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V \\]\n",
    "\n",
    "Where:\n",
    "- \\(Q = XW_Q\\), \\(K = XW_K\\), \\(V = XW_V\\) are the query, key, and value projections of the input \\(X\\).\n",
    "- \\(W_Q, W_K, W_V\\) are learnable weight matrices.\n",
    "\n",
    "#### **LoRA in Attention**\n",
    "LoRA introduces low-rank updates \\(\\Delta W_Q\\) and \\(\\Delta W_V\\) for \\(W_Q\\) and \\(W_V\\), respectively:\n",
    "\n",
    "\\[ W_Q' = W_Q + \\Delta W_Q \\quad \\text{and} \\quad W_V' = W_V + \\Delta W_V \\]\n",
    "\n",
    "Applying LoRA to both the query and value matrices ensures the model adapts effectively to the task, as these components directly control what information the model focuses on (queries) and how it retrieves knowledge (values).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Amplification, Not Addition**\n",
    "\n",
    "LoRA does not add new knowledge to the model. Instead, it **amplifies existing parameters** that are relevant for the specific task. This is why LoRA is described as a \"spotlight\" that highlights the necessary knowledge already present in the pre-trained model.\n",
    "\n",
    "This amplification is represented by:\n",
    "\n",
    "\\[ y = Wx + (AB^\\top)x \\]\n",
    "\n",
    "Here, \\((AB^\\top)x\\) selectively boosts specific components of the output \\(Wx\\).\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Computational and Storage Efficiency**\n",
    "\n",
    "#### **Storage Savings**\n",
    "In traditional fine-tuning, the entire weight matrix \\(W\\) is updated and stored for each task. With LoRA, only the low-rank matrices \\(A\\) and \\(B\\) are stored, reducing storage requirements dramatically. For example:\n",
    "- Full fine-tuning: 350 GB\n",
    "- LoRA fine-tuning: 35 MB\n",
    "\n",
    "#### **Computational Savings**\n",
    "LoRA reduces the number of trainable parameters and gradients, leading to faster training and lower memory usage. Additionally, since \\(W\\) is frozen, there’s no need to compute its gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Intuition: Library Analogy**\n",
    "\n",
    "Think of an LLM as a vast library:\n",
    "- Without LoRA: Searching for a specific book is like flipping through every page of every book (updating all parameters).\n",
    "- With LoRA: LoRA provides a targeted index, directing you to the exact shelf and book needed for the task.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Conclusion**\n",
    "\n",
    "LoRA leverages the low intrinsic rank of task-specific updates to fine-tune LLMs efficiently. By introducing low-rank updates to frozen weight matrices, it:\n",
    "- Dramatically reduces computational and storage costs.\n",
    "- Enhances task performance by amplifying existing model knowledge.\n",
    "- Reveals the structured nature of how LLMs represent and adapt knowledge.\n",
    "\n",
    "This method is particularly impactful in resource-constrained environments, enabling fine-tuning of massive models like GPT and BERT on commodity hardware.\n",
    "\n",
    "---\n",
    "\n",
    "### **Additional Insight: Rank of a Matrix**\n",
    "\n",
    "The rank of a matrix is the number of linearly independent vectors it contains. This means you cannot combine any of them linearly to produce another. It also indicates how many columns are redundant because redundant columns can be obtained by linearly combining others.\n",
    "\n",
    "For example, the weight matrix \\(W\\) is often rank-deficient, meaning it does not have full rank. If \\(W\\) has dimensions \\(1000 \\times 1000\\), but its actual rank is 10, most of the information in \\(W\\) can be captured using a \\(10 \\times 10\\) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **In-depth Explanation of LoRA (Low-Rank Adaptation) with Mathematical Foundations**\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is an efficient fine-tuning method for large language models (LLMs). Instead of updating all the parameters of the model, LoRA introduces a small set of additional parameters that are fine-tuned to adapt the model to a specific task. This reduces computational requirements and storage while maintaining high performance. Let’s explore it step-by-step with the necessary mathematical background.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Key Idea Behind LoRA**\n",
    "\n",
    "LLMs are typically parameterized by extremely large matrices, W, in each layer. These matrices define the weights that transform inputs to outputs across layers. Fine-tuning traditionally involves updating all these parameters for a specific task, which is computationally expensive and storage-heavy.\n",
    "\n",
    "LoRA’s key observation: **The task-specific updates to these large matrices have low intrinsic rank**. This means that instead of modifying the full W, the changes can be expressed as a low-rank decomposition.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Mathematical Formulation of LoRA**\n",
    "\n",
    "Let W be a weight matrix in the model. During fine-tuning, instead of directly updating W, LoRA expresses the update as:\n",
    "\n",
    "W' = W + ΔW\n",
    "\n",
    "Where:\n",
    "- W is the pre-trained weight matrix (frozen during LoRA fine-tuning).\n",
    "- ΔW is the task-specific update matrix, which LoRA approximates using a low-rank decomposition.\n",
    "\n",
    "#### **Low-Rank Decomposition**\n",
    "The update ΔW is parameterized as:\n",
    "\n",
    "ΔW = A × Bᵀ\n",
    "\n",
    "Where:\n",
    "- A is a low-rank matrix with dimensions (output size × rank).\n",
    "- B is a low-rank matrix with dimensions (input size × rank).\n",
    "- The rank, r, is much smaller than the dimensions of W, making ΔW a low-rank approximation.\n",
    "\n",
    "#### **Effective Parameter Reduction**\n",
    "The number of parameters introduced by LoRA is:\n",
    "\n",
    "Parameters in ΔW = r × (output size + input size)\n",
    "\n",
    "This is significantly smaller than the full (output size × input size) parameters of W.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Training with LoRA**\n",
    "\n",
    "During fine-tuning:\n",
    "- W is frozen, meaning it is not updated.\n",
    "- Only the parameters A and B are trained, which significantly reduces computational costs.\n",
    "\n",
    "The forward pass with LoRA becomes:\n",
    "\n",
    "y = W × x + (A × Bᵀ) × x\n",
    "\n",
    "Where:\n",
    "- x is the input vector.\n",
    "- y is the output vector.\n",
    "\n",
    "#### **Backpropagation**\n",
    "Only the gradients for A and B are computed during backpropagation, while W remains unchanged. This further reduces the memory and computational overhead.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Intrinsic Rank and Task-Specific Adaptation**\n",
    "\n",
    "#### **Intrinsic Rank Hypothesis**\n",
    "LoRA is based on the hypothesis that task-specific updates to a pre-trained LLM are inherently low-rank. This is because:\n",
    "- Pre-trained models already encode a vast amount of general knowledge.\n",
    "- Fine-tuning for a specific task typically requires only minor adjustments to highlight the relevant features.\n",
    "\n",
    "The rank, r, determines the expressiveness of the update. Empirical studies have shown that even very small values of r (e.g., r = 4 or r = 8) can lead to significant performance improvements.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Application in Self-Attention Mechanism**\n",
    "\n",
    "LoRA has been particularly effective when applied to the **query** and **value** matrices in the self-attention mechanism of transformers.\n",
    "\n",
    "#### **Attention Mechanism Overview**\n",
    "In transformers, the self-attention mechanism computes:\n",
    "\n",
    "Attention(Q, K, V) = softmax((Q × Kᵀ) / √dₖ) × V\n",
    "\n",
    "Where:\n",
    "- Q, K, V are the query, key, and value projections of the input, calculated as Q = X × Wₓ (and similarly for K and V).\n",
    "- Wₓ are learnable weight matrices.\n",
    "\n",
    "#### **LoRA in Attention**\n",
    "LoRA introduces low-rank updates ΔW_Q and ΔW_V for W_Q and W_V, respectively:\n",
    "\n",
    "W_Q' = W_Q + ΔW_Q  \n",
    "W_V' = W_V + ΔW_V\n",
    "\n",
    "Applying LoRA to both the query and value matrices ensures the model adapts effectively to the task, as these components directly control what information the model focuses on (queries) and how it retrieves knowledge (values).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Amplification, Not Addition**\n",
    "\n",
    "LoRA does not add new knowledge to the model. Instead, it **amplifies existing parameters** that are relevant for the specific task. This is why LoRA is described as a \"spotlight\" that highlights the necessary knowledge already present in the pre-trained model.\n",
    "\n",
    "Mathematically, this amplification is represented by:\n",
    "\n",
    "y = W × x + (A × Bᵀ) × x\n",
    "\n",
    "Here, (A × Bᵀ) × x selectively boosts specific components of the output W × x.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Computational and Storage Efficiency**\n",
    "\n",
    "#### **Storage Savings**\n",
    "In traditional fine-tuning, the entire weight matrix W is updated and stored for each task. With LoRA, only the low-rank matrices A and B are stored, reducing storage requirements dramatically. For example:\n",
    "- Full fine-tuning: 350 GB\n",
    "- LoRA fine-tuning: 35 MB\n",
    "\n",
    "#### **Computational Savings**\n",
    "LoRA reduces the number of trainable parameters and gradients, leading to faster training and lower memory usage. Additionally, since W is frozen, there’s no need to compute its gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Intuition: Library Analogy**\n",
    "\n",
    "Think of an LLM as a vast library:\n",
    "- Without LoRA: Searching for a specific book is like flipping through every page of every book (updating all parameters).\n",
    "- With LoRA: LoRA provides a targeted index, directing you to the exact shelf and book needed for the task.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Conclusion**\n",
    "\n",
    "LoRA leverages the low intrinsic rank of task-specific updates to fine-tune LLMs efficiently. By introducing low-rank updates to frozen weight matrices, it:\n",
    "- Dramatically reduces computational and storage costs.\n",
    "- Enhances task performance by amplifying existing model knowledge.\n",
    "- Reveals the structured nature of how LLMs represent and adapt knowledge.\n",
    "\n",
    "This method is particularly impactful in resource-constrained environments, enabling fine-tuning of massive models like GPT and BERT on commodity hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me break down LoRA (Low-Rank Adaptation) in a structured way that builds from fundamentals to advanced concepts.\n",
    "\n",
    "The Core Challenge\n",
    "Consider the challenge of adapting large language models. When we have a massive model like GPT-3 with hundreds of billions of parameters, we face several key problems. Each parameter is a floating-point number that needs to be adjusted during traditional fine-tuning. This means storing complete copies of the model for each adaptation, leading to enormous storage requirements – imagine needing 350GB for each version of the model. Beyond storage, the computational costs of adjusting billions of parameters are staggering, and loading multiple versions of a model becomes practically impossible.\n",
    "\n",
    "The Mathematical Insight\n",
    "This is where LoRA's elegance comes into play. Instead of modifying the entire weight matrix W (which transforms inputs in transformer models), LoRA introduces a clever mathematical decomposition. We add a small update matrix ΔW to the original weights, but here's the key insight: this update matrix can be broken down into the product of two much smaller matrices.\n",
    "\n",
    "Mathematically, it looks like this:\n",
    "W' = W + ΔW\n",
    "where ΔW = BA\n",
    "\n",
    "The matrices B and A are much smaller than W because they exploit a fundamental insight about how neural networks adapt to new tasks. B has dimensions d×r, and A has dimensions r×k, where r (the rank) is chosen to be much smaller than either d or k. Think of it like compressing a high-resolution image – while the original might have millions of pixels, you can often capture the essential features with much less information.\n",
    "\n",
    "Understanding Low Rank\n",
    "The concept of \"low rank\" is crucial but often misunderstood. In linear algebra, a matrix's rank represents its fundamental complexity – how many independent pieces of information it contains. Imagine trying to describe Earth's surface – while it exists in three dimensions, you only need two numbers (latitude and longitude) to specify any location. Similarly, LoRA suggests that the changes needed to adapt a language model for specific tasks have low intrinsic complexity.\n",
    "\n",
    "This isn't just about making small changes – it's about making precise, targeted modifications that leverage the model's existing knowledge structure. Think of it like adjusting the settings on a sophisticated camera rather than rebuilding its optics.\n",
    "\n",
    "Practical Implementation\n",
    "In a transformer model, we have several types of weight matrices: query projection (Wq), key projection (Wk), value projection (Wv), output projection (Wo), and feed-forward layers (W1 and W2). Through careful experimentation, researchers discovered that applying LoRA to both query and value matrices yields optimal results. This suggests these matrices play a special role in how models adapt to new tasks.\n",
    "\n",
    "The implementation includes several crucial details:\n",
    "\n",
    "First, we apply a scaling factor α to control the magnitude of adaptation:\n",
    "W' = W + α(BA)\n",
    "\n",
    "Second, we initialize the matrices A and B using a normal distribution with a small standard deviation. This ensures the adaptation starts small and grows naturally during training.\n",
    "\n",
    "Third, we typically choose ranks between 4 and 256, with 8 or 16 being common starting points. The choice of rank represents a balance – too small and it can't capture necessary adaptations, too large and we lose the benefits of parameter efficiency.\n",
    "\n",
    "Advanced Developments\n",
    "The success of LoRA has led to several important extensions. AdaLoRA dynamically adjusts the rank based on parameter importance, like a car's suspension automatically adjusting to road conditions. QLoRA combines LoRA with 4-bit quantization, further reducing memory requirements while maintaining performance.\n",
    "\n",
    "One particularly interesting property is the ability to compose multiple adaptations:\n",
    "W' = W + B₁A₁ + B₂A₂ + ...\n",
    "This means we can combine task-specific adaptations, like having multiple lens filters that can be combined for different photographic effects.\n",
    "\n",
    "Theoretical Implications\n",
    "LoRA's effectiveness reveals something profound about neural networks. The fact that such small, focused changes can significantly alter model behavior suggests these models might operate more like content-addressable memories than traditional programs. It's as if we're discovering that these massive neural networks have a hidden structure that makes them more manageable than their size would suggest.\n",
    "\n",
    "The success of targeting specific matrices (query and value) implies that knowledge in these models is organized in structured ways we're just beginning to understand. It's like discovering that a seemingly complex system actually follows simple, underlying principles.\n",
    "\n",
    "Future Directions\n",
    "Current research is exploring several exciting paths. Dynamic LoRA systems might automatically adjust their adaptation strategy based on the task at hand. Cross-model LoRA techniques could allow adaptations to transfer between different base models. Perhaps most importantly, researchers are working to develop better theoretical frameworks to explain why and when LoRA works so well.\n",
    "\n",
    "Practical Considerations\n",
    "When implementing LoRA, several factors require careful attention. The rank selection process involves balancing adaptability with efficiency. Learning rates can typically be larger than in full fine-tuning since fewer parameters are being updated. While LoRA dramatically reduces parameter count, attention mechanics still demand significant memory resources.\n",
    "\n",
    "Understanding LoRA this deeply not only helps in its practical application but also provides insights into the fundamental nature of large language models and how they can be efficiently adapted for specific tasks. It represents a breakthrough in our ability to work with these massive models, making them more practical and accessible while maintaining their powerful capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **In-depth Explanation of LoRA (Low-Rank Adaptation) with Mathematical Foundations**\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is an efficient fine-tuning method for large language models (LLMs). Instead of updating all the parameters of the model, LoRA introduces a small set of additional parameters that are fine-tuned to adapt the model to a specific task. This reduces computational requirements and storage while maintaining high performance. Let’s explore it step-by-step with the necessary mathematical background.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Key Idea Behind LoRA**\n",
    "\n",
    "LLMs are typically parameterized by extremely large matrices, \\( W \\), in each layer. These matrices define the weights that transform inputs to outputs across layers. Fine-tuning traditionally involves updating all these parameters for a specific task, which is computationally expensive and storage-heavy.\n",
    "\n",
    "LoRA’s key observation: **The task-specific updates to these large matrices have low intrinsic rank**. This means that instead of modifying the full \\( W \\), the changes can be expressed as a low-rank decomposition.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Mathematical Formulation of LoRA**\n",
    "\n",
    "Let \\( W \\in \\mathbb{R}^{d_{\\text{out}} \\times d_{\\text{in}}} \\) be a weight matrix in the model. During fine-tuning, instead of directly updating \\( W \\), LoRA expresses the update as:\n",
    "\n",
    "\\[\n",
    "W' = W + \\Delta W\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( W \\) is the pre-trained weight matrix (frozen during LoRA fine-tuning).\n",
    "- \\( \\Delta W \\) is the task-specific update matrix, which LoRA approximates using a low-rank decomposition.\n",
    "\n",
    "#### **Low-Rank Decomposition**\n",
    "The update \\( \\Delta W \\) is parameterized as:\n",
    "\n",
    "\\[\n",
    "\\Delta W = A B^\\top\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( A \\in \\mathbb{R}^{d_{\\text{out}} \\times r} \\) (low-rank matrix with rank \\( r \\)).\n",
    "- \\( B \\in \\mathbb{R}^{d_{\\text{in}} \\times r} \\) (low-rank matrix with rank \\( r \\)).\n",
    "- \\( r \\ll \\min(d_{\\text{out}}, d_{\\text{in}}) \\): The rank \\( r \\) is much smaller than the dimensions of \\( W \\), making \\( \\Delta W \\) a low-rank approximation.\n",
    "\n",
    "#### **Effective Parameter Reduction**\n",
    "The number of parameters introduced by LoRA is:\n",
    "\n",
    "\\[\n",
    "\\text{Parameters in } \\Delta W = r \\cdot (d_{\\text{out}} + d_{\\text{in}})\n",
    "\\]\n",
    "\n",
    "This is significantly smaller than the full \\( d_{\\text{out}} \\cdot d_{\\text{in}} \\) parameters of \\( W \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Training with LoRA**\n",
    "\n",
    "During fine-tuning:\n",
    "- \\( W \\) is frozen, meaning it is not updated.\n",
    "- Only the parameters \\( A \\) and \\( B \\) are trained, which significantly reduces computational costs.\n",
    "\n",
    "The forward pass with LoRA becomes:\n",
    "\n",
    "\\[\n",
    "y = W x + (A B^\\top) x\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( x \\in \\mathbb{R}^{d_{\\text{in}}} \\) is the input.\n",
    "- \\( y \\in \\mathbb{R}^{d_{\\text{out}}} \\) is the output.\n",
    "\n",
    "#### **Backpropagation**\n",
    "Only the gradients for \\( A \\) and \\( B \\) are computed during backpropagation, while \\( W \\) remains unchanged. This further reduces the memory and computational overhead.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Intrinsic Rank and Task-Specific Adaptation**\n",
    "\n",
    "#### **Intrinsic Rank Hypothesis**\n",
    "LoRA is based on the hypothesis that task-specific updates to a pre-trained LLM are inherently low-rank. This is because:\n",
    "- Pre-trained models already encode a vast amount of general knowledge.\n",
    "- Fine-tuning for a specific task typically requires only minor adjustments to highlight the relevant features.\n",
    "\n",
    "The rank \\( r \\) determines the expressiveness of the update. Empirical studies have shown that even very small values of \\( r \\) (e.g., \\( r = 4 \\) or \\( r = 8 \\)) can lead to significant performance improvements.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Application in Self-Attention Mechanism**\n",
    "\n",
    "LoRA has been particularly effective when applied to the **query** and **value** matrices in the self-attention mechanism of transformers.\n",
    "\n",
    "#### **Attention Mechanism Overview**\n",
    "In transformers, the self-attention mechanism computes:\n",
    "\n",
    "\\[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( Q = XW_Q \\), \\( K = XW_K \\), \\( V = XW_V \\) are the query, key, and value projections of the input \\( X \\).\n",
    "- \\( W_Q, W_K, W_V \\) are learnable weight matrices.\n",
    "\n",
    "#### **LoRA in Attention**\n",
    "LoRA introduces low-rank updates \\( \\Delta W_Q \\) and \\( \\Delta W_V \\) for \\( W_Q \\) and \\( W_V \\), respectively:\n",
    "\n",
    "\\[\n",
    "W_Q' = W_Q + \\Delta W_Q \\quad \\text{and} \\quad W_V' = W_V + \\Delta W_V\n",
    "\\]\n",
    "\n",
    "Applying LoRA to both the query and value matrices ensures the model adapts effectively to the task, as these components directly control what information the model focuses on (queries) and how it retrieves knowledge (values).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Amplification, Not Addition**\n",
    "\n",
    "LoRA does not add new knowledge to the model. Instead, it **amplifies existing parameters** that are relevant for the specific task. This is why LoRA is described as a \"spotlight\" that highlights the necessary knowledge already present in the pre-trained model.\n",
    "\n",
    "Mathematically, this amplification is represented by:\n",
    "\n",
    "\\[\n",
    "y = W x + (A B^\\top) x\n",
    "\\]\n",
    "\n",
    "Here, \\( (A B^\\top) x \\) selectively boosts specific components of the output \\( W x \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Computational and Storage Efficiency**\n",
    "\n",
    "#### **Storage Savings**\n",
    "In traditional fine-tuning, the entire weight matrix \\( W \\) is updated and stored for each task. With LoRA, only the low-rank matrices \\( A \\) and \\( B \\) are stored, reducing storage requirements dramatically. For example:\n",
    "- Full fine-tuning: \\( 350 \\, \\text{GB} \\)\n",
    "- LoRA fine-tuning: \\( 35 \\, \\text{MB} \\)\n",
    "\n",
    "#### **Computational Savings**\n",
    "LoRA reduces the number of trainable parameters and gradients, leading to faster training and lower memory usage. Additionally, since \\( W \\) is frozen, there’s no need to compute its gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Intuition: Library Analogy**\n",
    "\n",
    "Think of an LLM as a vast library:\n",
    "- Without LoRA: Searching for a specific book is like flipping through every page of every book (updating all parameters).\n",
    "- With LoRA: LoRA provides a targeted index, directing you to the exact shelf and book needed for the task.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Conclusion**\n",
    "\n",
    "LoRA leverages the low intrinsic rank of task-specific updates to fine-tune LLMs efficiently. By introducing low-rank updates to frozen weight matrices, it:\n",
    "- Dramatically reduces computational and storage costs.\n",
    "- Enhances task performance by amplifying existing model knowledge.\n",
    "- Reveals the structured nature of how LLMs represent and adapt knowledge.\n",
    "\n",
    "This method is particularly impactful in resource-constrained environments, enabling fine-tuning of massive models like GPT and BERT on commodity hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain LoRA (Low-Rank Adaptation) in more detail, clarifying some key concepts and correcting a few misconceptions from the text.\n",
    "\n",
    "LoRA is a parameter-efficient fine-tuning method that was introduced to make LLM adaptation more practical and efficient. Here's a detailed breakdown:\n",
    "\n",
    "Core Concept:\n",
    "- Instead of fine-tuning all parameters in a large language model (which could be billions of parameters), LoRA introduces small trainable \"rank decomposition\" matrices into the model's layers.\n",
    "- These matrices are much smaller than the original model weights but can effectively adapt the model's behavior for specific tasks.\n",
    "\n",
    "The Mathematics Behind It:\n",
    "- In a standard transformer model, we have weight matrices (W) that transform inputs.\n",
    "- LoRA decomposes the update to these weights (ΔW) into a product of two smaller matrices: ΔW = BA, where:\n",
    "  - B is a matrix of size (d × r)\n",
    "  - A is a matrix of size (r × k)\n",
    "  - r is the \"rank\" of the decomposition, typically much smaller than d and k\n",
    "  - This dramatically reduces the number of parameters that need to be trained\n",
    "\n",
    "The \"Low Rank\" Insight:\n",
    "- The \"low rank\" in LoRA refers to the discovery that effective model adaptations often have low intrinsic dimensionality.\n",
    "- This means that while the original model might have millions or billions of parameters, the changes needed to adapt it for a specific task can be captured in a much smaller space.\n",
    "- It's not just about making small changes, but about making precise, targeted modifications that leverage the model's existing knowledge.\n",
    "\n",
    "Implementation in Practice:\n",
    "- LoRA is typically applied to specific parts of the transformer architecture:\n",
    "  - The Query and Value matrices in the self-attention mechanism are the most common targets\n",
    "  - Research has shown that adapting both Query and Value matrices together provides better results than adapting either alone\n",
    "- The original model weights remain frozen, and only the LoRA parameters are trained\n",
    "\n",
    "Benefits:\n",
    "1. Storage Efficiency:\n",
    "   - Traditional fine-tuned models require storing a complete copy of the model weights\n",
    "   - LoRA only needs to store the small rank decomposition matrices\n",
    "   - This can reduce storage requirements from hundreds of gigabytes to mere megabytes\n",
    "\n",
    "2. Computational Efficiency:\n",
    "   - Training is faster because fewer parameters need to be updated\n",
    "   - The low-rank nature of the updates means less computational overhead during inference\n",
    "\n",
    "3. Performance:\n",
    "   - Despite its efficiency, LoRA often achieves comparable or better performance than full fine-tuning\n",
    "   - The targeted nature of the updates can help prevent catastrophic forgetting\n",
    "\n",
    "Knowledge Representation:\n",
    "- LoRA's success suggests that LLMs organize knowledge in structured ways\n",
    "- Rather than adding new information, LoRA often works by emphasizing or reprioritizing existing knowledge within the model\n",
    "- This is more like adjusting the focus or attention of the model rather than teaching it entirely new concepts\n",
    "\n",
    "Practical Applications:\n",
    "- Task-specific adaptations (e.g., medical domain specialization)\n",
    "- Language adaptation\n",
    "- Personality or style tuning\n",
    "- Instruction following improvements\n",
    "\n",
    "Limitations:\n",
    "- Not all tasks benefit equally from LoRA\n",
    "- The optimal rank size can vary by task and model\n",
    "- May not be as effective for tasks requiring substantial new knowledge\n",
    "\n",
    "This understanding of LoRA has led to various extensions and improvements, such as QLoRA (Quantized LoRA) and AdaLoRA (Adaptive LoRA), which further optimize the approach for different scenarios and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain LoRA (Low-Rank Adaptation) comprehensively by breaking it down into stages of understanding.\n",
    "\n",
    "First, let's understand traditional neural network training and fine-tuning:\n",
    "\n",
    "In a typical neural network, information flows from input through multiple hidden layers to produce an output. When we train the network, we compare this output to our target (ground truth) to calculate loss, then use backpropagation to adjust all the weights throughout the network. Think of it like tuning every string on a massive piano – each weight needs individual attention and adjustment.\n",
    "\n",
    "When we want to adapt a pre-trained model to new data, we traditionally use fine-tuning. However, this approach faces three major challenges:\n",
    "\n",
    "The first challenge is computational cost. Modern language models can have billions of parameters, and updating all of them requires immense computational power. Imagine trying to repaint an entire skyscraper – it requires massive resources and time.\n",
    "\n",
    "The second challenge is memory constraints. During training, we need to store not just the model weights, but also their gradients and optimizer states in GPU memory. This is like needing enough physical space to lay out and work on millions of puzzle pieces simultaneously.\n",
    "\n",
    "The third and most subtle challenge is catastrophic forgetting. When we modify all weights, the model might forget its original learning while adapting to new tasks. It's similar to how a person might forget their native language while intensively learning a new one.\n",
    "\n",
    "Now, let's understand how LoRA solves these problems:\n",
    "\n",
    "LoRA introduces a brilliant insight: instead of modifying the entire weight matrix W, we can approximate the changes we want to make using two smaller matrices, A and B. Let's use concrete numbers to understand this:\n",
    "\n",
    "Suppose we have a weight matrix W that is 1000 × 5000, containing 5 million parameters. Instead of modifying all these parameters, LoRA creates:\n",
    "- Matrix B: 1000 × r\n",
    "- Matrix A: r × 5000\n",
    "where r is a small number like 5 or 10\n",
    "\n",
    "The key mathematical insight is that when we multiply B and A, we get a matrix of the same size as W. So BA can be added to W to modify the model's behavior. It's like having a small adjustment panel that can control a much larger system.\n",
    "\n",
    "Using r = 5 as an example:\n",
    "- Original parameters: 1000 × 5000 = 5,000,000\n",
    "- LoRA parameters: (1000 × 5) + (5 × 5000) = 30,000\n",
    "\n",
    "This brings several powerful benefits:\n",
    "\n",
    "1. Parameter Efficiency: We're only training 30,000 parameters instead of 5 million – a 99.4% reduction! This makes training much faster and more efficient.\n",
    "\n",
    "2. Memory Savings: Since we're only storing and updating two small matrices, the memory requirements are drastically reduced. This allows us to use larger batch sizes or train on less powerful hardware.\n",
    "\n",
    "3. Quick Task Switching: For different tasks (like Java vs SQL), we only need to switch out the small A and B matrices rather than loading entire new copies of W. Think of it like switching lenses on a camera rather than buying a new camera for each type of photography.\n",
    "\n",
    "4. Original Knowledge Preservation: Since W remains frozen, we don't risk catastrophic forgetting. The model maintains its original capabilities while learning new ones.\n",
    "\n",
    "The brilliant insight of LoRA is recognizing that many weights in large models are redundant for specific tasks. By using low-rank approximation through matrices A and B, we can capture the essential adaptations we need while dramatically reducing the computational and memory burden.\n",
    "\n",
    "Would you like me to elaborate on any particular aspect, such as how the rank r is chosen or how LoRA handles multiple layers in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll help explain LoRA (Low-Rank Adaptation) and why it's such an important innovation for fine-tuning large language models. Let's break this down step by step.\n",
    "\n",
    "First, let's understand the traditional fine-tuning approach and its challenges:\n",
    "\n",
    "In traditional neural network training and fine-tuning, we start with a model that has multiple layers of weights. When we fine-tune the model, we need to adjust all these weights through backpropagation to adapt the model to new data. This creates several significant problems:\n",
    "\n",
    "1. The computational cost is enormous because we're updating millions or billions of parameters. Imagine trying to carefully adjust every knob in a massive control room simultaneously - it requires tremendous computing power and time.\n",
    "\n",
    "2. Memory becomes a major constraint because we need to store not just the model weights, but also their gradients and optimizer states in GPU memory. This is like needing enough desk space to spread out and work on every page of a thousand books at once.\n",
    "\n",
    "3. Most concerningly, the model can experience \"catastrophic forgetting\" - like a student who forgets their basic math while learning calculus. The model might lose its previously learned capabilities while adapting to new tasks.\n",
    "\n",
    "This is where LoRA comes in with an elegant solution. Looking at the diagram, here's how LoRA works:\n",
    "\n",
    "Instead of modifying the original model's weights (matrix W), LoRA introduces two smaller matrices, A and B, while keeping the original model frozen. The key insight is mathematical: if W is a d×k matrix (say 1000×5000), instead of modifying all 5 million parameters, we can create:\n",
    "- Matrix B: d×r matrix (1000×r)\n",
    "- Matrix A: r×k matrix (r×5000)\n",
    "- Where r is much smaller than both d and k\n",
    "\n",
    "The magic happens because:\n",
    "1. When we multiply A and B, we get a matrix of the same size as W\n",
    "2. We can add this result to the original W matrix's output\n",
    "3. We only need to train A and B, while keeping W frozen\n",
    "\n",
    "Let me use an analogy: Imagine W is like a massive library catalog system. Instead of reorganizing the entire catalog (traditional fine-tuning), LoRA is like adding a small appendix (matrices A and B) that efficiently describes the changes we want to make. The original catalog stays intact, but we can still effectively update the system's behavior.\n",
    "\n",
    "The benefits are profound:\n",
    "- Instead of training millions of parameters, we might only train a few thousand\n",
    "- Memory usage drops dramatically\n",
    "- The original model's knowledge is preserved since we never modify W\n",
    "- We can store different A and B matrices for different tasks, making the model adaptable without conflicts\n",
    "\n",
    "The equation r << min(d,k) in the diagram is crucial - it tells us that r should be much smaller than both d and k, ensuring we maintain the efficiency benefits of this approach.\n",
    "\n",
    "This innovation has made it practical for researchers and developers to fine-tune large language models on limited hardware, democratizing access to AI technology while maintaining good performance. Would you like me to elaborate on any particular aspect of LoRA, such as how the matrices A and B are initialized or how the training process works in more detail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets\n",
    "# ! pip install transformers\n",
    "# ! pip install peft\n",
    "# ! pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # weights and boases comes enabled by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    Qwen2VLForConditionalGeneration,\n",
    "    Qwen2VLProcessor,\n",
    "    BitsAndBytesConfig,  # quatinzation\n",
    ")\n",
    "\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")  # as we dont have enough gpu to work with full vision model so we are gpoing with adapaters such as LoRa\n",
    "\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(f\"{device=}\")\n",
    "\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "GRADIENT_CHECKPOINTING = True\n",
    "USE_REENTRANT = False\n",
    "OPTIM = \"paged_adamw_32bit\"\n",
    "LEARNING_RATE = 2e-5\n",
    "LOGGING_STEPS = 50\n",
    "EVAL_STEPS = 50\n",
    "SAVE_STEPS = 50\n",
    "SAVE_STRATEGY = \"steps\"\n",
    "EVAL_STRATEGY = \"steps\"\n",
    "METRIC_FOR_BEST_MODEL = (\n",
    "    \"eval_loss\"  # to get better model which might not be the last saved model\n",
    ")\n",
    "LOAD_BEST_MODEL_AT_END = True\n",
    "MAX_GRAD_NORM = 1\n",
    "WARMUP_STEPS = 0\n",
    "DATASET_KWARGS = {\n",
    "    \"skip_prepare_dataset\": True\n",
    "}  # we have to put for VLM    # we are preparnig data oujrselvess\n",
    "\n",
    "REMOVE_UNUSED_COLUMNS = False  # VLM thing\n",
    "MAX_SEQ_LEN = 128  # max seq len of the generated text\n",
    "DATA_POINTS_IN_DATASET = 283\n",
    "NUM_STEPS = (DATA_POINTS_IN_DATASET // BATCH_SIZE) * EPOCHS\n",
    "print(f\"{NUM_STEPS=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fomrat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our data will need to have format that is compatible with VLM and SIFT trainer\n",
    "# so for every data point in dataset we will the points in a fomrat that we want\n",
    "# for that first we'll display a system_message\n",
    "\n",
    "system_message = \"\"\" You are a highly advanced Vision Language Model (VLM), specializing in analyzing, describing and interpreting visual data.\n",
    "Your task is to process and extract meaningful insights from images, videos and visual patterns,\n",
    "leveraging multimodal understanding to provide accurate and contextually relevant infomration.\"\"\"\n",
    "print(f\"{system_message=}\")\n",
    "\n",
    "\n",
    "def format_data(sample):\n",
    "    # print(f\"input_sample: {sample}\")\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": system_message},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",  # input\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
    "                {\"type\": \"text\", \"text\": sample[\"query\"]},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"][0]}],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset, test_dataset = load_dataset(\n",
    "    \"HuggingFaceM4/ChartQA\",\n",
    "    split=[\"train[:1%]\", \"val[:1%]\", \"test[:1%]\"],  # only using 1% of data\n",
    ")\n",
    "print(f\"{train_dataset=} \\n {test_dataset=} \\n {eval_dataset=}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"\\n\\n\\n{train_dataset[0]=} \\n\\n {test_dataset[0]=} \\n\\n {eval_dataset[0]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0][\"image\"]  # reading image directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[11][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[11][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets f\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_data(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [format_data(sample=sample) for sample in train_dataset]\n",
    "eval_dataset = [format_data(sample=sample) for sample in eval_dataset]\n",
    "test_dataset = [format_data(sample=sample) for sample in test_dataset]\n",
    "len(train_dataset), len(eval_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet you shared prepares a dataset for fine-tuning a Vision Language Model (VLM) like **Qwen/Qwen2-VL-7B-Instruct**. Let's break it down to explain why certain steps are necessary:\n",
    "\n",
    "---\n",
    "\n",
    "### **Why the System Message?**\n",
    "The `system_message` is a way to define the role and task of the VLM. By explicitly stating that the model specializes in visual and textual analysis, it sets the context for the interactions during training or inference. This is especially important when working with large language models or VLMs that rely on multimodal input, as the system message serves as a prompt to steer the model's behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Format the Data?**\n",
    "The function `format_data(sample)` is crucial for structuring the dataset into a format compatible with the model's expected input. The VLM expects data in a structured sequence of **system**, **user**, and **assistant** roles, which mimic the conversational flow. Here's why each part is important:\n",
    "\n",
    "1. **System Role**:\n",
    "   - Defines the context or instructions for the model.\n",
    "   - Ensures consistency in behavior across training and inference.\n",
    "\n",
    "2. **User Role**:\n",
    "   - Contains the input data the model must process.\n",
    "   - Includes:\n",
    "     - An **image**: The visual data for the VLM to interpret.\n",
    "     - A **text query**: The specific question or task related to the image.\n",
    "\n",
    "3. **Assistant Role**:\n",
    "   - Contains the model's expected output (label or answer).\n",
    "   - This is the target the model tries to predict during training.\n",
    "\n",
    "---\n",
    "\n",
    "### **Dataset Details**\n",
    "You are loading the **ChartQA** dataset, which consists of questions about charts and graphs. Each data point includes:\n",
    "- An `image` (chart/graph).\n",
    "- A `query` (the question about the chart).\n",
    "- A `label` (the correct answer).\n",
    "- A `human_or_machine` flag (indicating the source of the data).\n",
    "\n",
    "The dataset is split into train, validation, and test sets, with only 1% of the data being used for faster experimentation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use `format_data(train_dataset[0])`?**\n",
    "The `format_data` function transforms each data point into the structured format the VLM expects during training or evaluation. For example, for the first training sample:\n",
    "\n",
    "Input:\n",
    "```python\n",
    "{\n",
    "    'image': <PIL image>,\n",
    "    'query': 'Is the value of Favorable 38 in 2015?',\n",
    "    'label': ['Yes'],\n",
    "    'human_or_machine': 0\n",
    "}\n",
    "```\n",
    "\n",
    "Transformed Output:\n",
    "```python\n",
    "[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": system_message}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": <PIL image>},\n",
    "            {\"type\": \"text\", \"text\": 'Is the value of Favorable 38 in 2015?'}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": 'Yes'}]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "This transformation is necessary because Qwen/Qwen2-VL models are trained to process data in this multimodal, role-based format. The `format_data` function ensures that the training data aligns with the model's architecture and training objectives.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Is This Necessary for Fine-Tuning?**\n",
    "1. **Consistency**: Fine-tuning requires data in the same format as the pretraining phase to ensure compatibility.\n",
    "2. **Multimodal Alignment**: The model processes both images and text, so the data must be structured to pair the visual and textual components correctly.\n",
    "3. **Role-Based Interaction**: The system-user-assistant structure is integral to the conversational and task-solving nature of instruction-tuned VLMs.\n",
    "\n",
    "---\n",
    "\n",
    "If you're fine-tuning **Qwen/Qwen2-VL-7B-Instruct**, the `format_data` function prepares the dataset in a way that aligns with the model's multimodal and conversational architecture, ensuring that it learns effectively from the given examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structuring training data for Vision Language Model (VLM) fine-tuning in a format like the example you provided is crucial for several reasons:\n",
    "\n",
    "### 1. **Standardized Multimodal Input Format**\n",
    "   - VLMs are designed to process multimodal data, i.e., text, images, and their combinations. Structuring the data explicitly defines the type and content of each modality (`'text'`, `'image'`, etc.).\n",
    "   - This standardization helps the model understand how to parse and relate different data types, ensuring compatibility with its architecture and training objectives.\n",
    "\n",
    "### 2. **Role-based Context Management**\n",
    "   - Roles such as `'system'`, `'user'`, and `'assistant'` help delineate the context of the conversation or task.\n",
    "   - This structure enables the model to distinguish between system instructions, user queries, and its own responses, maintaining clarity in multi-turn interactions.\n",
    "\n",
    "### 3. **Modality Alignment**\n",
    "   - Each example explicitly ties visual data (e.g., the image) with corresponding textual descriptions or queries. This alignment helps the model learn the relationship between visual and textual inputs, which is the core of vision-language understanding.\n",
    "\n",
    "### 4. **Support for Complex Interactions**\n",
    "   - By defining the interaction flow, the model can handle tasks that require sequential reasoning, such as interpreting an image, answering questions, or generating captions.\n",
    "   - It also allows for chaining interactions, enabling models to retain and build on previous context.\n",
    "\n",
    "### 5. **Facilitates Loss Computation**\n",
    "   - During fine-tuning, the model learns to predict outputs based on the input context. A structured format helps clearly define inputs (e.g., `'user'` role) and expected outputs (e.g., `'assistant'` role), making it easier to compute loss and train the model effectively.\n",
    "\n",
    "### 6. **Interoperability with Model APIs**\n",
    "   - Many modern VLMs (like GPT-style VLMs or CLIP-based models) require input in specific formats to process multimodal data. Structuring data this way ensures compatibility with these APIs and models.\n",
    "\n",
    "### 7. **Flexibility for Expansion**\n",
    "   - This format is extensible. For example, additional modalities (e.g., audio, video) or metadata (e.g., timestamps, user IDs) can be added without disrupting the existing structure.\n",
    "   - It supports future scenarios, like adding follow-up questions, explanations, or detailed analyses.\n",
    "\n",
    "### 8. **Improved Training Efficiency**\n",
    "   - Structured data ensures that the training pipeline can efficiently preprocess, batch, and feed data into the model. It reduces ambiguity and preprocessing errors during training.\n",
    "\n",
    "### Example in Practice:\n",
    "For the given structure:\n",
    "```python\n",
    "[\n",
    "  {'role': 'system', 'content': [{'type': 'text', 'text': '...'}]},\n",
    "  {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.Image object>}, {'type': 'text', 'text': '...'}]},\n",
    "  {'role': 'assistant', 'content': [{'type': 'text', 'text': 'Yes'}]}\n",
    "]\n",
    "```\n",
    "\n",
    "- The `'system'` role sets the task context.\n",
    "- The `'user'` role defines multimodal input (image + text query).\n",
    "- The `'assistant'` role specifies the model's expected response.\n",
    "\n",
    "This format ensures the VLM learns to process visual data in the context of textual queries and generate coherent, task-specific outputs.\n",
    "\n",
    "### Summary\n",
    "Structuring data this way is necessary to align with the design principles of multimodal models, ensure compatibility with their architectures, and enable efficient, meaningful training for tasks requiring visual and textual understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset preprocessing done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = test_dataset[0]\n",
    "print(f\"{sample_data=}\")\n",
    "print(\"-\" * 30)\n",
    "sample_question = sample_data[1][\"content\"][1][\"text\"]\n",
    "print(f\"{sample_question=}\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "sample_answer = sample_data[2][\"content\"][0][\"text\"]\n",
    "print(f\"{sample_answer=}\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "sample_image = sample_data[1][\"content\"][0][\"image\"]\n",
    "print(f\"sample image:\")\n",
    "sample_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "if device == \"cuda\":\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        use_cache=False,  # kv cache\n",
    "    )\n",
    "else:\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        use_cache=False,  # kv cache\n",
    "    )\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading processor\n",
    "processor = Qwen2VLProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\"  # for auto regressive model, it is good rpactice ti set padding side to right\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(sample_data):\n",
    "    # first remove answer from sample whihc is at second instance .. i.e. assistan\n",
    "    text_without_answer = sample_data[0:2]\n",
    "    print(f\"{text_without_answer=}\")  # no answer included\n",
    "\n",
    "    text = processor.apply_chat_template(\n",
    "        text_without_answer,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    print(f\"\\n\\nPrompt: {text}\")\n",
    "    \"\"\"\n",
    "    This prints the formatted prompt after applying the chat template. The prompt is structured in a way that the model can understand, including:\n",
    "\n",
    "    The system message.\n",
    "\n",
    "    The user input (image placeholder + text question).\n",
    "\n",
    "    A placeholder for the assistant's response.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ----\n",
    "\n",
    "    Prompt: <|im_start|>system\n",
    "    You are a highly advanced Vision Language Model (VLM), specializing in analyzing, describing and interpreting visual data.\n",
    "    Your task is to process and extract meaningful insights from images, videos and visual patterns,\n",
    "    leveraging multimodal understanding to provide accurate and contextually relevant information.<|im_end|>\n",
    "    <|im_start|>user\n",
    "    <|vision_start|><|image_pad|><|vision_end|>How many food items are shown in the bar graph?<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # image input\n",
    "    image_inputs = sample_data[1][\"content\"][0][\"image\"]\n",
    "    print(f\"{image_inputs=}\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    inputs = processor(\n",
    "        text=[text], images=image_inputs, return_tensors=\"pt\"  # pytorch tensor\n",
    "    )\n",
    "\n",
    "    # print(f\"{inputs=}\")\n",
    "    print(\n",
    "        \"\"\"\\n\n",
    "        generated input:\n",
    "            input_ids: Tokenized text input (as a tensor).\n",
    "            attention_mask: Attention mask for the text input (as a tensor).\n",
    "            pixel_values: Processed image data (as a tensor).\n",
    "            image_grid_thw: Image grid dimensions (as a tensor).\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # putin git on device\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    # now generate text\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=MAX_SEQ_LEN)\n",
    "    print(f\"{generated_ids=}\")\n",
    "    # to make it sensible\n",
    "    print(\"-\" * 30)\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    print(f\"{output_text=}\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    del inputs\n",
    "\n",
    "    actual_answer = sample_data[2][\"content\"][0][\"text\"]\n",
    "    return output_text[0], actual_answer\n",
    "\n",
    "\n",
    "# # Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "generated_text, actual_answer = text_generator(sample_data)\n",
    "\n",
    "print(f\"{generated_text=}\")\n",
    "print(f\"{actual_answer=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actual_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model genraeteas assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generated input\n",
    "input_ids: Tokenized text input (as a tensor).\n",
    "\n",
    "attention_mask: Attention mask for the text input (as a tensor).\n",
    "\n",
    "pixel_values: Processed image data (as a tensor).\n",
    "\n",
    "image_grid_thw: Image grid dimensions (as a tensor).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a JSON-like representation of a multimodal input and task for a Vision Language Model (VLM). Let’s break it down step by step:\n",
    "\n",
    "### 1. **System Role and Context**\n",
    "   - **Role:** `system`\n",
    "   - **Content:** The system's description sets the context for the VLM's capabilities:\n",
    "     - Specializes in analyzing, describing, and interpreting visual data.\n",
    "     - Processes images, videos, and visual patterns to extract insights.\n",
    "     - Provides accurate and contextually relevant information using multimodal understanding.\n",
    "\n",
    "### 2. **User Role and Input**\n",
    "   - **Role:** `user`\n",
    "   - **Content:**\n",
    "     - **Image Input:** A bar graph image represented as a `PIL.PngImagePlugin.PngImageFile` object with dimensions 850x600 in RGBA mode. This indicates the image is ready for processing.\n",
    "     - **Text Query:** \"How many food item is shown in the bar graph?\" This is the task the user wants the VLM to perform.\n",
    "\n",
    "### 3. **Prompt Representation**\n",
    "   - **`<|im_start|>` and `<|im_end|>`:** These delimiters indicate the start and end of input sections for the VLM. The sections include:\n",
    "     - System instructions.\n",
    "     - User inputs (both visual and textual).\n",
    "\n",
    "### 4. **Encoded Input Details**\n",
    "   - The **image** is included as an input object (`image_inputs`) for processing.\n",
    "   - The **text query** is tokenized into `input_ids` for the model to process alongside the image.\n",
    "   - The encoding appears to include placeholders and tokenized information used for multimodal processing.\n",
    "\n",
    "### 5. **Task**\n",
    "   - The model is tasked with interpreting the bar graph and answering the query about the number of food items displayed in it.\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation of Specific Details:\n",
    "1. **PIL Image Object:** This shows that the input image is being handled by the Python Imaging Library (PIL), indicating a preprocessing step before feeding the image to the model.\n",
    "2. **Tokenized Inputs:** The `input_ids` contain the tokenized version of the text query, formatted for the model’s multimodal processing.\n",
    "\n",
    "### Output Expectation:\n",
    "The VLM will:\n",
    "1. Analyze the provided bar graph image.\n",
    "2. Count the number of food items depicted.\n",
    "3. Return the result as a textual response to the user's query.\n",
    "\n",
    "Let me know if you'd like to delve deeper into any specific part of this input structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output represents the input and processing steps for a Vision-Language Model (VLM). Here's a concise breakdown:\n",
    "\n",
    "1. **System Role**: The model is defined as a Vision-Language Model specializing in analyzing visual data and providing insights.\n",
    "   \n",
    "2. **User Input**:\n",
    "   - An **image** (a bar graph, loaded using the PIL library) with metadata indicating size and mode.\n",
    "   - A **text query**: *\"How many food items are shown in the bar graph?\"*\n",
    "\n",
    "3. **Prompt Details**:\n",
    "   - The system and user inputs are structured using specific tags (`<|im_start|>`, `<|vision_start|>`) to process multimodal inputs.\n",
    "   - The query asks the model to interpret the image and provide a relevant response.\n",
    "\n",
    "4. **Intermediate Tokens**:\n",
    "   - The `input_ids` represent the tokenized form of the combined image and text input, which the model processes for generating the output.\n",
    "\n",
    "In essence, this output outlines how the VLM processes an image and text query together, encoding them into a unified representation for interpretation and response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,  # for scaling\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "# before and after putting adapters\n",
    "\n",
    "\n",
    "print(f\"Before adapter parameters:{ model.num_parameters()}\")\n",
    "# as we will be freezing original weights and just training adpters, we exexpect rtainable paramters to be a samll number\n",
    "\n",
    "peft_model = get_peft_model(\n",
    "    model, peft_config=peft_config\n",
    ")  # this will convert our model into LoRA adapter model\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_checkpointing=GRADIENT_CHECKPOINTING,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    eval_strategy=EVAL_STRATEGY,\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    metric_for_best_model=METRIC_FOR_BEST_MODEL,\n",
    "    load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    dataset_kwargs=DATASET_KWARGS,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    remove_unused_columns=REMOVE_UNUSED_COLUMNS,\n",
    "    optim=OPTIM,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "    EPOCHS = 1\n",
    "    BATCH_SIZE = 1\n",
    "    GRADIENT_CHECKPOINTING = True\n",
    "    USE_REENTRANT = False\n",
    "    OPTIM = \"paged_adamw_32bit\"\n",
    "    LEARNING_RATE = 2e-5\n",
    "    LOGGING_STEPS = 50\n",
    "    EVAL_STEPS = 50\n",
    "    SAVE_STEPS = 50\n",
    "    SAVE_STRATEGY = \"steps\"\n",
    "    EVAL_STRATEGY = \"steps\"\n",
    "    METRIC_FOR_BEST_MODEL = (\n",
    "        \"eval_loss\"  # to get better model which might not be the last saved model\n",
    "    )\n",
    "    LOAD_BEST_MODEL_AT_END = True\n",
    "    MAX_GRAD_NORM = 1\n",
    "    WARMUP_STEPS = 0\n",
    "    DATASET_KWARGS = {\n",
    "        \"skip_prepare_dataset\": True\n",
    "    }  # we have to put for VLM    # we prepared data ourselves\n",
    "\n",
    "    REMOVE_UNUSED_COLUMNS = False  # VLM thing\n",
    "    MAX_SEQ_LEN = 128  # max seq len of the generated text\n",
    "    DATA_POINTS_IN_DATASET = 283\n",
    "    NUM_STEPS = (DATA_POINTS_IN_DATASET // BATCH_SIZE) * EPOCHS\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# as we have said in DATASET_KWARGS that we will do data preparations ourselves\n",
    "# we need to pass the fuction that will do data preparation into trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will return dict that will have input_ids, attention_mask, pixel values and labels\n",
    "collate_sample = [train_dataset[0], train_dataset[1]]  # for batch size 2\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # we will also need to pass answer\n",
    "    texts = [\n",
    "        processor.apply_chat_template(example, tokenize=False) for example in examples\n",
    "    ]\n",
    "    image_inputs = [example[1][\"content\"][0][\"image\"] for example in examples]\n",
    "\n",
    "    batch = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,  # since it is batch\n",
    "    )\n",
    "\n",
    "    print(f\"Before: {batch.keys()=}\")\n",
    "\n",
    "    labels = batch[\n",
    "        \"input_ids\"\n",
    "    ].clone()  # input_ids are token ids.. and we need ignore padding token\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    batch[\"labels\"] = batch[\"input_ids\"]\n",
    "    print(f\"labels added{batch.keys()=}\")\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "collated_data = collate_fn(collate_sample)\n",
    "collated_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet is designed to prepare a dataset for training a model, particularly a Vision-Language Model (VLM) like **Qwen2**, which processes both text and image data. The goal is to create a batch of text and image data, tokenize the text, and format it in a way suitable for model input.\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "1. **Creating a Sample Batch (`collate_sample`)**:\n",
    "   ```python\n",
    "   collate_sample = [train_dataset[0], train_dataset[1]]  # for batch size 2\n",
    "   ```\n",
    "   - `collate_sample` is a list containing two examples (`train_dataset[0]` and `train_dataset[1]`).\n",
    "   - This will be used as a batch of size 2. Each example is assumed to be a tuple containing text and image data.\n",
    "   \n",
    "2. **Collate Function Definition (`collate_fn`)**:\n",
    "   ```python\n",
    "   def collate_fn(examples):\n",
    "   ```\n",
    "   - `collate_fn` is the function that processes a batch of data (in this case, two samples) and prepares it for input to the model.\n",
    "\n",
    "3. **Processing Text Data**:\n",
    "   ```python\n",
    "   texts = [\n",
    "       processor.apply_chat_template(example, tokenize=False) for example in examples\n",
    "   ]\n",
    "   ```\n",
    "   - `processor.apply_chat_template` is applied to each example in `examples` to prepare the text data.\n",
    "   - `tokenize=False` indicates that the text should not be tokenized at this stage; tokenization will happen later (during the batch processing).\n",
    "\n",
    "4. **Extracting Image Data**:\n",
    "   ```python\n",
    "   image_inputs = [example[1][\"content\"][0][\"image\"] for example in examples]\n",
    "   ```\n",
    "   - `image_inputs` extracts the image data from each example. It assumes the structure of each example is a tuple where the second element is a dictionary with a key `\"content\"`, which holds the image data.\n",
    "\n",
    "5. **Processing Text and Image Inputs**:\n",
    "   ```python\n",
    "   batch = processor(\n",
    "       text=texts,\n",
    "       images=image_inputs,\n",
    "       return_tensors=\"pt\",\n",
    "       padding=True,  # since it is batch\n",
    "   )\n",
    "   ```\n",
    "   - `processor` is likely an instance of a tokenizer and image processor (e.g., from Hugging Face's `transformers` library). It processes both text and image inputs.\n",
    "   - The `text` argument contains the list of text data (`texts`), and `images` contains the image data (`image_inputs`).\n",
    "   - `return_tensors=\"pt\"` specifies that the output should be in PyTorch tensor format.\n",
    "   - `padding=True` ensures that text sequences are padded to the same length for the batch.\n",
    "\n",
    "6. **Printing Batch Keys Before Labeling**:\n",
    "   ```python\n",
    "   print(f\"Before: {batch.keys()=}\")\n",
    "   ```\n",
    "   - This prints the keys of the `batch` dictionary before labels are added. These keys will likely include things like `input_ids`, `attention_mask`, and `pixel_values`.\n",
    "\n",
    "7. **Creating Labels**:\n",
    "   ```python\n",
    "   labels = batch[\"input_ids\"].clone()\n",
    "   labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "   batch[\"label\"] = batch[\"input_ids\"]\n",
    "   ```\n",
    "   - A clone of the `input_ids` (the tokenized text) is made to create the `labels`.\n",
    "   - Any padding tokens in the `input_ids` are replaced with `-100`. This is because in most NLP models, `-100` is used to mask out padding tokens during loss computation, so they are not considered in the backpropagation.\n",
    "   - The `labels` are then added to the `batch` dictionary under the key `\"label\"`. This ensures that the model has the correct labels to compute loss during training.\n",
    "\n",
    "8. **Printing Batch Keys After Labeling**:\n",
    "   ```python\n",
    "   print(f\"label added{batch.keys()=}\")\n",
    "   ```\n",
    "   - This prints the keys of the `batch` dictionary after the `label` has been added. The batch now contains keys for the input text (`input_ids`), attention masks (`attention_mask`), image pixel values (`pixel_values`), and the `labels`.\n",
    "\n",
    "9. **Returning the Processed Batch**:\n",
    "   ```python\n",
    "   return batch\n",
    "   ```\n",
    "   - The function returns the processed batch, which now contains:\n",
    "     - `input_ids`: The tokenized text data.\n",
    "     - `attention_mask`: A mask that tells the model which tokens are padding.\n",
    "     - `pixel_values`: The processed image data.\n",
    "     - `labels`: The labels used for training, where padding tokens are masked out with `-100`.\n",
    "\n",
    "10. **Using the Collate Function**:\n",
    "    ```python\n",
    "    collated_data = collate_fn(collate_sample)\n",
    "    collated_data.keys()\n",
    "    ```\n",
    "    - The `collate_fn` is called with the sample batch `collate_sample`.\n",
    "    - The `keys()` method prints the keys of the resulting dictionary (`collated_data`), which will contain the keys for the processed text and image data (`input_ids`, `attention_mask`, `pixel_values`, `labels`).\n",
    "\n",
    "### Summary of the Process:\n",
    "- **Input**: A batch of two examples, each containing text and image data.\n",
    "- **Text Processing**: The text is processed using a custom method (`apply_chat_template`), without tokenizing at this point.\n",
    "- **Image Processing**: The images are extracted and prepared for the model.\n",
    "- **Batch Preparation**: The text and image data are passed through a processor that tokenizes the text, processes the images, and pads the sequences.\n",
    "- **Labels**: A copy of the tokenized text (`input_ids`) is used as labels, with padding tokens replaced by `-100`.\n",
    "- **Output**: A batch dictionary is returned, containing the processed text (`input_ids`), image pixel values (`pixel_values`), attention masks (`attention_mask`), and labels (`label`).\n",
    "\n",
    "### Expected Output of `collated_data.keys()`:\n",
    "- `input_ids`: Tokenized IDs of the text data.\n",
    "- `attention_mask`: Mask to indicate non-padding tokens.\n",
    "- `pixel_values`: Processed image data (likely as tensors).\n",
    "- `labels`: The same as `input_ids`, but with padding tokens masked out (`-100`).\n",
    "\n",
    "This function is essential for preparing the dataset in a format that the model can process, ensuring the correct handling of both text and image inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,  # regular mogel not LoRA\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inital evaluation of the model without any training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 30)\n",
    "print(\"Initial evaluation\")\n",
    "metric = trainer.evaluate()\n",
    "print(f\"{metric=}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining\")\n",
    "trainer.train()\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clearing out memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/learn/cookbook/en/fine_tuning_vlm_trl\n",
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "\n",
    "\n",
    "# loading model\n",
    "if device == \"cuda\":\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        use_cache=True,  # kv cache can be used in inference\n",
    "    )\n",
    "else:\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        use_cache=True,  # kv cache\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mounitng loRa adapters onto the model\n",
    "print(f\"parameters before loading adapters: {model.num_parameters()}\")\n",
    "model.load_adapter(\"./output\")\n",
    "\n",
    "print(f\"parameters before loading adapters: {model.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text\n",
    "generated_text, actual_answer = text_generator(sample_data=sample_data)\n",
    "print(f\"generated text: {generated_text}\")\n",
    "print(f\"actual answer: {actual_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***https://huggingface.co/learn/cookbook/en/fine_tuning_vlm_trl***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Bits and Bytes** library by Hugging Face is a lightweight utility for optimizing the memory usage and performance of large language models (LLMs). It focuses on quantization techniques (like 8-bit and 4-bit quantization) to reduce model size and accelerate inference while maintaining accuracy, making it ideal for deploying large models on resource-constrained devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Bits and Bytes** library by Hugging Face helps with fine-tuning large language models by enabling **low-bit quantization**, such as 8-bit or 4-bit, which reduces the memory footprint and computational requirements. Here's how it supports fine-tuning:\n",
    "\n",
    "1. **Efficient Memory Usage**: Quantization reduces the size of model weights, allowing fine-tuning of larger models on GPUs with limited VRAM.\n",
    "\n",
    "2. **Faster Training**: Lower precision computations are faster, speeding up the fine-tuning process without significantly sacrificing model performance.\n",
    "\n",
    "3. **Parameter-Efficient Fine-Tuning (PEFT)**: It supports methods like **LoRA** (Low-Rank Adaptation) and other PEFT techniques, enabling you to fine-tune only specific parts of the model (e.g., adapters) while keeping the rest of the model frozen, further reducing resource demands.\n",
    "\n",
    "4. **Wide Model Support**: Works seamlessly with popular Hugging Face Transformers, allowing users to quantize and fine-tune a wide range of pre-trained models.\n",
    "\n",
    "In summary, **Bits and Bytes** makes fine-tuning more accessible by reducing hardware requirements and enabling efficient training workflows for large-scale models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
